{
  "articleName":"LLMs can hide text in other text of the same length",
  "articleText":"## Curious Secrets Hiding in Plain Text\n\nA new study by researchers at the University of Oxford reveals that language models (LLMs) are capable of hiding text within other texts of the same length. This means that seemingly innocent sentences can hold secret messages, raising concerns about potential misuse and the true nature of online content.\n\nImagine writing \"The government has repeatedly failed.\" but concealing this message within another sentence: \"The infamous British roasted boar with mint sauce. How to make it perfect.\" By following a sequence generated by an AI, your secret message can be discovered. This technique isn't just for entertainment; there are serious implications as well.\n\nOne possible scenario is a company using an AI tool that appears harmless but secretly provides illegal instructions in seemingly \"safe answers\". The 'safe answer' might sound okay, but the true content could be hidden and potentially harmful.\n\nThis discovery changes our perception of text on the internet. A review of a video game may hide a political manifesto or an email could contain corporate espionage. Text is no longer just what it seems; it could unlock hidden meanings.\n\nNext time you read something online, be aware that the writer might be playing tricks and hiding secret messages!",
  "articleTags":["AI","LARGE LANGUAGE MODELS","NATURAL LANGUAGE PROCESSING","AI SAFETY","AI ETHICS"],
  "articleUrl": "http://arxiv.org/pdf/2510.20075v1",
  "date": "2025-11-21"
}