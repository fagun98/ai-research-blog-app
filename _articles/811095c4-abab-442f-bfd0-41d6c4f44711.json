{
  "articleName": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation",
  "articleText": "## The Hidden Dangers of AI-Driven Science: Can We Trust Our Results?\n\nHave you ever wondered how researchers use artificial intelligence (AI) to analyze huge amounts of text, like social media posts or news articles? A new study reveals a surprising truth: even the best AI models can produce flawed results if not used carefully.\n\nImagine you're trying to find out if people's attitudes are changing over time. You could ask an AI to analyze thousands of tweets and label them as positive or negative. But what if the AI gives you a different answer depending on how you word your question? That's exactly what this study found: small changes in the way you ask the AI can lead to drastically different results.\n\nThe researchers tested their ideas by redoing 21 published studies using 18 different AI models and labeling over 13 million pieces of text. They found that even the best models were wrong about a third of the time, while smaller models got it wrong half the time. And if someone intentionally wanted to manipulate the results, they could easily do so by trying out different combinations of AI models and prompts.\n\nBut here's the good news: there are simple solutions to this problem! For one thing, researchers can involve human experts in labeling a small number of texts \u2013 just 100 can be enough. This approach is more reliable than relying on AI alone. Additionally, researchers should make their methods transparent by registering their choices before starting an analysis.\n\nSo what does this mean for you? When you see headlines about new studies that used AI to analyze text, remember: these results might not be as reliable as you think. Ask yourself if the researchers validated their tools carefully. By being aware of these limitations, we can promote more rigorous and trustworthy research in the age of AI.",
  "articleTags": [
    "AI",
    "MACHINE LEARNING",
    "NATURAL LANGUAGE PROCESSING",
    "ARTIFICIAL INTELLIGENCE SAFETY",
    "DATA PRIVACY"
  ],
  "articleUrl": "http://arxiv.org/pdf/2509.08825v1\n",
  "date": "2025-09-16"
}