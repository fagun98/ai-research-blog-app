{
  "articleName": "Large-Scale, Longitudinal Study of Large Language Models During the 2024 US Elec...",
  "articleText": "## AI Election Predictions: What We Learned from the 2024 US Election Study\n\nImagine you're trying to guess who will win an election by asking a magic crystal ball for its predictions. Sounds like fun, right? But what if that crystal ball is actually a powerful computer program called a Large Language Model (LLM)? Researchers from Stanford and MIT studied how these AI models predicted the 2024 US election, and their findings are both fascinating and unsettling.\n\n### Main Points\n\n* The researchers tested 12 different LLMs by asking them over 12,000 questions about politics every day for 4 months.\n* They found that these AI models change their answers over time, sometimes in surprising ways. For example, an LLM might say it can't predict election results before September 3rd but can after that date!\n* The researchers also discovered that users can \"steer\" the AI's responses by providing demographic information, such as age or party affiliation. This means that the way we ask questions affects what answers we get.\n\n### Examples\n\n* Imagine asking an LLM, \"What is the importance of healthcare in the US?\" It might answer differently depending on whether you say you're a Democrat or Republican.\n* The researchers found that Donald Trump was often associated with adjectives like divisive and corrupt, while Kamala Harris was linked to compassionate and honorable.\n\n### Wrap-up\n\nThis study shows us that AI tools are not neutral calculators but complex systems that can change in invisible ways. They're sensitive to how we ask questions and contain internal models of the world that can lead to biased answers. While this is both fascinating and unsettling, it's essential for us to understand how these technologies shape our perceptions of reality.",
  "articleTags": [
    "AI SAFETY",
    "ALIGNMENT",
    "LARGE LANGUAGE MODELS"
  ],
  "articleUrl": "http://arxiv.org/pdf/2509.18446v1",
  "date": "2025-09-27"
}