{
  "articleName":"CoDA: Coding LM via Diffusion Adaptation",
  "articleText":"## Tiny but Mighty: How Small AI Models Can Write Code Like Pros\n\nImagine you're writing a story, and your friend takes out some of the words. You'd still be able to understand what's going on, right? That's basically how small AI models called diffusion language models (DLMs) work. They can fill in missing code with surprising accuracy!\n\n### The Big Problem with Big Models\n\nMost AI models for writing code are like super-long stories that need a lot of space and time to finish. But what if we told you that smaller models, like the 1.7 billion parameter KOD model, can do just as well? They're like tiny but mighty superheroes!\n\n### How Did They Make It Work?\n\nThe researchers tested their ideas by training the small KOD model on a huge dataset of code and web text. Then, they used a clever technique called \"progressive masking\" to make it learn how to fill in missing blocks of code. This allowed KOD to become incredibly good at infilling tasks, like filling in a missing function body!\n\n### The Results Were Amazing!\n\nIn tests, the KOD model beat bigger models on several key metrics and even outperformed some of them. It was especially great at editing and refactoring code, which is where bigger models tend to struggle. And here's the cool part: it did all this while being way faster and more efficient than its bigger competitors!\n\n### What Does This Mean for Us?\n\nThis paper shows that we don't need massive AI models to get good results. Smaller DLMs can be just as effective, if not better! This opens up new possibilities for real-time coding assistance and could make AI more accessible to people who want to learn how to code.\n\nSo, the next time you see a small but mighty AI model, remember: it's like a tiny superhero that can save the day with its incredible coding skills!",
  "articleTags":["AI","MACHINE LEARNING","DEEP LEARNING"],
  "articleUrl": "https://arxiv.org/pdf/2510.03270",
  "date": "2025-10-12"
}