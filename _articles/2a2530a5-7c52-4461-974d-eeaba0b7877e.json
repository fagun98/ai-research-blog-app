{
  "articleName": "Evidently AI: What is prompt injection? Example attacks, defenses and testing",
  "articleText": "## The Sneaky Problem of Prompt Injection\n\nImagine you're talking to a super smart AI that can summarize long documents or even write your emails for you. Sounds cool, right? But what if someone tricks the AI into saying something it shouldn't? That's basically what happens with \"prompt injection.\"\n\n### What is Prompt Injection?\n\nIt's like a game of mix-and-match where untrusted input (like what a user types in) gets mixed up with trusted instructions (what you, the developer, told the system to do). The AI gets confused and follows the wrong instructions. It's like telling your friend \"be nice\" but then them saying something mean anyway.\n\n### How Does it Happen?\n\nWell, large language models (LLMs) are super good at processing text, but they have a problem: they can't tell what's trusted input and what's not. So if someone types in some sneaky instructions that look like a normal conversation, the AI might just follow those instead of your original plan.\n\n### Why is this a Big Deal?\n\nIt's a huge risk because it can lead to all sorts of problems. For example, an attacker could trick the AI into sending out emails it shouldn't or even deleting important data. It's like having a super smart but also super trusting best friend who might do something silly if you asked them to.\n\n### So What Can We Do About it?\n\nTo prevent prompt injection, we need to think ahead and design systems with safety in mind. Here are some ideas:\n\n*   **Least Privilege**: Give the AI only the permissions it needs to get the job done.\n*   **Human-in-the-loop**: Have a person review and approve important actions before they happen.\n*   **Retrieve without Generate**: Don't feed raw untrusted text directly into the AI's prompt. Instead, use it as reference material that users can look at.\n\n### In Short...\n\nPrompt injection is like a sneaky game of mix-and-match where attackers try to trick the AI into doing something bad. To prevent this, we need to design systems with safety in mind and have multiple layers of defense. By being vigilant and testing our systems regularly, we can minimize the damage if an injection attempt succeeds.\n\n### Why Does it Matter?\n\nUnderstanding prompt injection matters because it's a real-world problem that can affect anyone using LLMs. It's not just about techies or developers; it's about regular people who use these AI-powered tools every day. By being aware of this risk, we can build safer and more secure systems for everyone.",
  "articleTags": [
    "AI SECURITY",
    "CYBERSECURITY",
    "DATA PRIVACY",
    "ARTIFICIAL INTELLIGENCE SAFETY",
    "LARGE LANGUAGE MODELS"
  ],
  "articleUrl": "NONE",
  "date": "2025-10-23"
}